# 网络IO模型

缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，**数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间**。

在Linux中，对于一次I/O**读取**的操作，数据并不会直接拷贝到程序的程序缓冲区，通常包括两个不同阶段：

1. 等待数据准备好，到达内核空间 (Waiting for the data to be ready) ；
2. 从内核向进程复制数据 (Copying the data from the kernel to the process)

对于一次I/O写入的操作，和上面是类似的，过程相反。

## 同步阻塞IO（blocking IO）

<img src="https://hl1998-1255562705.cos.ap-shanghai.myqcloud.com/Img/v2-8caf9a2d0a49c3d72cddb40de7f2979e_1440w.webp" alt="img" style="zoom: 33%;" />

伪代码如下：

```cpp
int main()
{
    //...
    read(socket, buffer);   //阻塞在此，等待内核返回数据
    process(buffer);
    //...
}
```

## 同步非阻塞IO（nonblocking IO)

<img src="https://hl1998-1255562705.cos.ap-shanghai.myqcloud.com/Img/v2-6d7f042c99e0117df576b4e3e09f22f9_1440w.webp" alt="img" style="zoom: 33%;" />

伪代码如下

```cpp
int main(){
    //...
    while(read(socket,buffer) != success){
        //轮询等待，会浪费cpu资源
    }
    //...
    process(buffer)
}
```



## IO多路复用（IO multiplexing）

本质是把轮询这个动作放在内核中去做了，程序阻塞在select、poll、epoll上。

<img src="https://hl1998-1255562705.cos.ap-shanghai.myqcloud.com/Img/v2-fba6c2868dc5d8e1f369c09e785ace61_1440w.webp" alt="img" style="zoom:33%;" />



伪代码如下：

```cpp
int main(){
    //...
    select(socket);
    while(1){
        sockets = select();         //数据已经存在内核缓冲区了,需要拷贝到用户态。
        for(socket in sockets) {
            if(can_read(socket)) {
                read(socket, buffer);
                process(buffer);
            }else if(can_write(socket)){
                write(socket, buffer);
                process(buffer);
            }else{
                // ....
            }
        }
    }
    //...
}
```

## 信号驱动IO（signal driven IO）

不怎么使用

## 异步IO（asynchronous IO）

太复杂




# 服务端网络模式

## thread-based

### 方案0：阻塞IO，单进程-单线程模型。

服务端一次只能服务一个客户(最原始的socket收发流程)

进程要接受tcp连接必然要调用accept并且被阻塞，直到有一条连接到达，在这之前无法做别的事情，也即是说单个进程一次只能处理一条连接，业务处理完成之后调用close关闭连接，然后继续等待accept，循环往复，这种情况下是无法实现高并发的，所以一般会使用多进程再来同时处理更多的连接

### 方案1：阻塞IO，多进程-单线程模型。

**情况1：accept后再fork子进程**

父进程accept后，fork子进程，因为子进程会**复制父进程的文件描述符**，所以子进程可以和客户端通信。

这个是最简单的模式，由于只有一个进程在使用accept进行监听，不涉及多进程争抢的问题，当tcp连接事件到达后也只会唤醒这个监听进程，自然也不存在惊群效应。

这种模式就是来一个请求起一个进程，当并发量上来后，机器是撑不住的(内存cpu等资源不够)。

**情况2：fork子进程后再accpet**

预先fork，就是进程池(相对于线程池)

这种情况下，因为所有子进程都在监听父进程创建的listenfd，所以当有连接到来时，所有子进程都会被唤醒，但是只有一个进程能获取到事件并进行处理。这就出现了惊群问题

好在2.6版本的内核已经解决了，当有连接事件到来时候，只有一个进程或者线程的accept会被唤醒，不会出现惊群问题。

这种模式下(可以简单理解成进程池模式)，当子进程在处理业务请求的时候，就无法接受连接，所以并发效率不高。好处是不用频繁创建和销毁进程。

<img src="https://hl1998-1255562705.cos.ap-shanghai.myqcloud.com/Img/91c46be0a0e90c7f532e890a78793c9a.webp" alt="img" style="zoom: 67%;" />

### 方案2：阻塞IO，单进程-多线程模型。

该方案和1没啥太大的区别，就是换成了线程而已。

- [对应情况1]主线程accept后，创建一个线程来处理，完事之后线程自己被干掉。好处是当前的请求不会影响后来的请求，但是并发一大还是撑不住。
- [对应情况2]主线程accept后，将acceptfd放入队列中，通过条件变量等通知**线程池**中的子线程去队列中获取fd，然后子线程和客户端通信。好处是不用频繁创建线程，但是当线程繁忙时，会影响后续请求。问题在于阻塞IO，子线程要阻塞在read上等客户端发送数据来，如果客户端不发数据，就会一直阻塞住。



线程/进程模型一般都是阻塞IO，如果换成非阻塞IO来看，因为不知道什么时候有连接到来或是有数据到来，所以得while轮询，浪费cpu资源，假设可以知道什么时候有连接到来或者数据可读呢？

linux下面epoll/select/poll系统调用正是这个功能，由epoll、select、poll系统调用引出了事件驱动。

## envent-driven

基于事件驱动的设计思想，有Reactor模式和Proactor模式。

Reactor模式：**有一个或多个并发输入源，有一个Service Handler，有多个Request Handlers**；Service Handler会对输入的请求（Event）进行多路复用，并同步地将它们分发给相应的Request Handler。

### 方案0：非阻塞IO+IO多路复用，单进程-单线程模型。

该方案也可以叫做**Reactor模式-单线程模式**，所有的IO操作和业务逻辑都在主线程中完成。主线程阻塞在select/poll/epoll上，当有事件发生时(新链接、数据读写)，Reactor进行事件分发，链接事件时丢给acceptor类，调用accept()完成tcp三次握手，然后让select/poll/epoll关注acceptfd的读事件，当数据到来的时候丢给业务类去处理，处理完后回复对端，再让select/poll/epoll关注acceptfd的写事件来进行发送。

<img src="https://hl1998-1255562705.cos.ap-shanghai.myqcloud.com/Img/a68d2959ed2039ac6c3585e0459295d0.webp" alt="img" style="zoom: 67%;" />

优点：编程简单，对于业务处理不复杂的后台服务，基本能够满足需求

缺点：会有阻塞服务器，在进行业务处理的时候不能进行其他操作：如建立连接，读取其他套接字上的数据等。

所以一般为了高并发，还是会采用多进程/线程的模式

### 方案1：非阻塞IO+IO多路复用，单进程-多线程模式

**情况1：epoll_create后再fork | 线程共用一个epfd**

这种情况下，所有的子线程共用一个epfd，那么当有事件发生的时候，所有的epoll_wait都会被唤醒。

还有一个问题是，比如说，A进程在epoll挂了socket1的连接事件，B进程调用了epoll_wait，由于属于同一个epfd，当socket1产生事件的时候，进程B也会被唤醒，而更严重的事情在于，在B的空间下并不存在socket1这个fd，从而把问题搞得很复杂。

<font color='red'>这种方案是不考虑的</font>

**情况2：fork之后再epoll_create | 线程各自拥有一个epfd**

这种情况就是每个线程都单独有一个epfd。如果我们只需要处理 accept 事件的话，貌似世界一片美好了。但是 epoll 并不是只处理 accept 事件，accept 后续的读写事件都需要处理，还有定时或者信号事件。

当连接到来时，我们需要选择一个进程来 accept，这个时候，任何一个 accept 都是可以的。当连接建立以后，后续的读写事件，却与进程有了关联。**一个请求与 a 进程建立连接后，后续的读写也应该由 a 进程来做**。

当读写事件发生时，应该通知哪个进程呢？epoll 并不知道，因此，事件有可能错误通知另一个进程，这是不对的。所以一般在每个进程（线程）里面会再次创建一个 epoll 事件循环机制，每个进程的读写事件只注册在自己进程的 epoll 中。

如何处理accept有几种方案：

1，一个进程专门负责处理连接。可以是父进程accept后，通过round robin方式然子进程的epoll来监听后续的读写事件。

2，使用SO_REUSEPORT，这样任意一个进程都能处理accept请求，完事以后还能继续在本线程中监听后续的读写事件

3，使用EPOLLEXCLUSIVE，epoll_wait返回的可能只有一个(因为`EPOLLEXCLUSIVE` 只保证唤醒的进程数小于等于我们开启的进程数，而不是直接唤醒所有进程，也不是只保证唤醒一个进程。)，然后去处理accept。

#### a) mainReactor+subReactor+threadpoll

该模式主要是将Reactor进行了拆分，做单一职责。mainReactor主要处理链接事件，处理完成后通知subReactor等待读写事件，subReactor将read的数据丢给工作线程做业务逻辑，等工作线程处理完成后再send。

<img src="https://hl1998-1255562705.cos.ap-shanghai.myqcloud.com/Img/v2-14b10c1dd4c45a1fe3fd92f91fffe2e3_1440w.webp" alt="img" style="zoom:50%;" />

#### b) one thread one loop

该模式是a的变种，仍然保留线程池，不过主要的做法是将IO操作和业务逻辑都和线程进行了绑定。不用subReactor做专门的数据读取工作，每个线程自己就是一个subReactor。mainReactor和之前一样，仍然负责处理链接事件，也就是通过accept接受连接，完成链接的建立后，通过eventfd通知subReactor，并将acceptfd传递给subReactor(每一个subReactor都有自己的eventfd，所以通知的时候不会有惊群问题)，subReactor就只需要等待数据到来，调用read读取数据做业务，然后再调用send发送数据。

#### c) one thread one loop+SO_REUSEPORT

该模式去掉了mainReactor的概念，每一个loop自己负责accept和read/write。当使用SO_REUSEPORT后，所有loop都监听一个listenfd，但内核会只唤醒某一个loop有新链接事件(epoll_wait结束阻塞)。这种模式相当于方案3的升级版本，能充分利用cpu，但是存在的问题也如ed-方案0一般。