# 网络IO模型

缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，**数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间**。

在Linux中，对于一次I/O**读取**的操作，数据并不会直接拷贝到程序的程序缓冲区，通常包括两个不同阶段：

1. 等待数据准备好，到达内核空间 (Waiting for the data to be ready) ；
2. 从内核向进程复制数据 (Copying the data from the kernel to the process)

对于一次I/O写入的操作，和上面是类似的，过程相反。

## 同步阻塞IO（blocking IO）

<img src="https://hl1998-1255562705.cos.ap-shanghai.myqcloud.com/Img/v2-8caf9a2d0a49c3d72cddb40de7f2979e_1440w.webp" alt="img" style="zoom: 33%;" />

伪代码如下：

```cpp
int main()
{
    //...
    read(socket, buffer);	//阻塞在此，等待内核返回数据
    process(buffer);
    //...
}
```

## 同步非阻塞IO（nonblocking IO)

<img src="https://hl1998-1255562705.cos.ap-shanghai.myqcloud.com/Img/v2-6d7f042c99e0117df576b4e3e09f22f9_1440w.webp" alt="img" style="zoom: 33%;" />

伪代码如下

```cpp
int main(){
    //...
	while(read(socket,buffer) != success){
		//轮询等待，会浪费cpu资源
	}
    //...
	process(buffer)
}
```



## IO多路复用（IO multiplexing）

本质是把轮询这个动作放在内核中去做了，程序阻塞在select、poll、epoll上。

<img src="https://hl1998-1255562705.cos.ap-shanghai.myqcloud.com/Img/v2-fba6c2868dc5d8e1f369c09e785ace61_1440w.webp" alt="img" style="zoom:33%;" />



伪代码如下：

```cpp
int main(){
	//...
    select(socket);
    while(1){
        sockets = select();			//数据已经存在内核缓冲区了,需要拷贝到用户态。
        for(socket in sockets) {
            if(can_read(socket)) {
                read(socket, buffer);
                process(buffer);
            }else if(can_write(socket)){
                write(socket, buffer);
                process(buffer);
            }else{
                // ....
            }
        }
    }
    //...
}
```

## 信号驱动IO（signal driven IO）

不怎么使用

## 异步IO（asynchronous IO）

太复杂




# 服务端网络模式

## thread-based

### 方案0：阻塞IO，单进程-单线程模型。

服务端一次只能服务一个客户(最原始的socket收发流程)

### 方案1：阻塞IO，多进程-单线程模型。

父进程accept后，fork子进程，因为子进程会**复制父进程的文件描述符**，所以子进程可以和客户端通信。

<img src="https://hl1998-1255562705.cos.ap-shanghai.myqcloud.com/Img/91c46be0a0e90c7f532e890a78793c9a.webp" alt="img" style="zoom: 67%;" />

### 方案2：阻塞IO，单进程-多线程模型。

- 主线程accept后，将acceptfd放入队列中，通过条件变量等通知**线程池**中的子线程去队列中获取fd，然后子线程和客户端通信。好处是不用频繁创建线程，但是当线程繁忙时，会影响后续请求。问题在于阻塞IO，子线程要阻塞在read上等客户端发送数据来，如果客户端不发数据，就会一直阻塞住。
- 主线程accept后，创建一个线程来处理，完事之后线程自己被干掉。好处是不会当前的请求不会影响后来的请求，但是并发一大还是撑不住。

线程模型一般都是阻塞IO，如果换成非阻塞IO来看，因为不知道什么时候read有数据，所以得while轮询，浪费cpu资源，假设可以知道什么时候read返回呢？linux下面epoll/select/poll系统调用正是这个功能，由epoll、select、poll系统调用引出了事件驱动。

## envent-driven

基于事件驱动的设计思想，有Reactor模式和Proactor模式。

Reactor模式是事件驱动的一种情况，**有一个或多个并发输入源，有一个Service Handler，有多个Request Handlers**；Service Handler会对输入的请求（Event）进行多路复用，并同步地将它们分发给相应的Request Handler。

### 方案3：非阻塞IO+IO多路复用，单进程-单线程模型。

该方案也可以叫做**Reactor模式-单线程模式**，所有的IO操作和业务逻辑都在主线程中完成。主线程阻塞在select/poll/epoll上，当有事件发生时(新链接、数据读写)，Reactor进行事件分发，链接事件时丢给acceptor类，调用accept()完成tcp三次握手。数据读写则丢给业务类去处理。



<img src="https://hl1998-1255562705.cos.ap-shanghai.myqcloud.com/Img/a68d2959ed2039ac6c3585e0459295d0.webp" alt="img" style="zoom: 67%;" />

优点：编程简单，对于业务处理不复杂的后台服务，基本能够满足需求

缺点：会有阻塞服务器，在进行业务处理的时候不能进行其他操作：如建立连接，读取其他套接字上的数据等。

### 方案4：非阻塞IO+IO多路复用，单进程-工作者线程池模型

与单线程模式不同的是，添加了一个**工作者线程池**，并将非I/O操作(业务逻辑)从Reactor线程中移出转交给工作者线程池（Thread Pool）来执行。这样能够提高Reactor线程的I/O响应，不至于因为一些耗时的业务逻辑而延迟对后面I/O请求的处理。

虽然非I/O操作交给了线程池来处理，但是**所有的I/O操作依然由Reactor单线程执行**，在高负载、高并发或大数据量的应用场景，依然较容易成为瓶颈。

<img src="https://hl1998-1255562705.cos.ap-shanghai.myqcloud.com/Img/497f3ceeaffc512997dbe6bd4943652b.webp" alt="img" style="zoom:67%;" />

### 方案5：非阻塞IO+IO多路复用，单进程-多线程模式

该模式也有不同的处理思路：

#### 5.1 mainReactor+subReactor+threadpoll

该模式主要是将Reactor进行了拆分，做单一职责。mainReactor主要处理链接事件，处理完成后通知subReactor等待读写事件，subReactor将read的数据丢给工作线程做业务逻辑，等工作线程处理完成后再send。

<img src="https://hl1998-1255562705.cos.ap-shanghai.myqcloud.com/Img/v2-14b10c1dd4c45a1fe3fd92f91fffe2e3_1440w.webp" alt="img" style="zoom:50%;" />

#### 5.2 one thread one loop

该模式是5.1的变种，仍然保留线程池，不过主要的做法是将IO操作和业务逻辑都和线程进行了绑定。不用subReactor做专门的数据读取工作，每个线程自己就是一个subReactor。mainReactor和之前一样，仍然负责处理链接事件，处理完成后通知所有的subReactor(每一个subReactor都有自己的eventfd，所以通知的时候不会有惊群问题)做业务

#### 5.3 one thread one loop+SO_REUSEPORT

该模式去掉了mainReactor的概念，每一个loop自己负责accept和read/write。当使用SO_REUSEPORT后，所有loop都监听一个listenfd，但内核会只唤醒某一个loop有新链接事件(epoll_wait结束阻塞)。这种模式相当于方案3的升级版本，能充分利用cpu，但是存在的问题也如方案3一般。

### 方案6：非阻塞IO+IO多路复用，多进程-单线程模式

Nginx的网络模型，早期通过accept_mutex避免惊群问题，后期使用SO_REUSEPORT，由内核来负责唤醒某一个阻塞在epoll_wait上的线程。主进程只负责管理，子进程做具体的业务，这样子某一个子进程挂了也不影响其他的子进程(子线程挂了，整个进程都会挂，因为操作系统认为你不安全了)。和方案5.3相比，这种更加安全。